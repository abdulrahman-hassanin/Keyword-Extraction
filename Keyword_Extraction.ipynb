{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8YmU1Ihyf+pw7JbffFdQT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdulrahman-hassanin/Keyword-Extraction/blob/main/Keyword_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keyword Extraction \n",
        "\n",
        "Keyword extraction algorithms are methods that find relevant keywords or key phrases from text documents. They are useful for summarizing, indexing, and searching text documents. There are different types of keyword extraction algorithms, such as:\n",
        "\n",
        "- **Statistical methods**: They compute statistics for keywords and use those statistics to score them. Some examples are word frequency, word collocation, co-occurrence, TF-IDF, and YAKE.\n",
        "\n",
        "- **Graph-based methods**: They build a graph of words or phrases and use graph algorithms to rank them. Some examples are TextRank, Multi-word Keyword Scoring Strategy, ExpandRank, PositionRank, and Word Attraction Rank.\n",
        "\n",
        "- **Deep-Learning-Embedding-based methods**: They are based on semantic similarity not statistcs properties by using word or phrase embeddings to measure the similarity and importance of keywords such as KeyBERT."
      ],
      "metadata": {
        "id": "KU34EHEykcAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\" The dominant sequence transduction models are based on complex recurrent or\n",
        "convolutional neural networks that include an encoder and a decoder. The best\n",
        "performing models also connect the encoder and decoder through an attention\n",
        "mechanism. We propose a new simple network architecture, the Transformer,\n",
        "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
        "entirely. Experiments on two machine translation tasks show these models to\n",
        "be superior in quality while being more parallelizable and requiring significantly\n",
        "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including\n",
        "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
        "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
        "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
        "best models from the literature. We show that the Transformer generalizes well to\n",
        "other tasks by applying it successfully to English constituency parsing both with\n",
        "large and limited training data.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "TY6DH6xy4eWR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yake\n",
        "!pip install git+https://github.com/boudinfl/pke.git\n",
        "!pip install rake-nltk\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "uqK_X25B5cpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statical Methods\n",
        "\n",
        "The simplest method. It Uses the statistics for keywords to score them.\n",
        "\n",
        "- **TF-IDF**: Estimates word importance relative to the entire corpus.\n",
        "- **YAKE**: uses statistical features from a single document to extract keywords.\n"
      ],
      "metadata": {
        "id": "iomYEe360TAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yake \n",
        "\n",
        "def yake_extractor(text):\n",
        "    \"\"\"\n",
        "    Uses YAKE to extract the top 5 keywords from a text\n",
        "    Arguments: text (str)\n",
        "    Returns: list of keywords (list)\n",
        "    \"\"\"\n",
        "    keywords = yake.KeywordExtractor(lan=\"en\", n=3, windowsSize=3, top=5).extract_keywords(text)\n",
        "    results = []\n",
        "    for keyword, score in keywords:\n",
        "        results.append(keyword)\n",
        "    return results \n",
        "\n",
        "yake_extractor(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b78135rH4dCV",
        "outputId": "0920a499-3965-46d9-c63b-ea03da8cc1c1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dominant sequence transduction',\n",
              " 'convolutional neural networks',\n",
              " 'sequence transduction models',\n",
              " 'dominant sequence',\n",
              " 'sequence transduction']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph-based methods\n",
        "Graph-based methods generate a graph of related terms from the documents. A graph, for example, connects terms that co-occur in the text.\n",
        "\n",
        "\n",
        "*   **PositionRank**: Compute Candidates using Part-Of-Speech tagging, then weight them using the position\n",
        "\n",
        "*   **RAKE**: based on the observation that keywords are frequently composed of multiple words and usually do not include the stop-words or punctuations."
      ],
      "metadata": {
        "id": "WGdHpl4U6u8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pke\n",
        "\n",
        "def position_rank_extractor(text):\n",
        "    \"\"\"\n",
        "    Uses PositionRank to extract the top 5 keywords from a text\n",
        "    Arguments: text (str)\n",
        "    Returns: list of keywords (list)\n",
        "    \"\"\"\n",
        "    # define the valid Part-of-Speeches to occur in the graph\n",
        "    pos = {'NOUN', 'PROPN', 'ADJ', 'ADV'}\n",
        "    extractor = pke.unsupervised.PositionRank()\n",
        "    extractor.load_document(text, language='en')\n",
        "    extractor.candidate_selection(maximum_word_number=5)\n",
        "    # 4. weight the candidates using the sum of their word's scores that are\n",
        "    #    computed using random walk biaised with the position of the words\n",
        "    #    in the document. In the graph, nodes are words (nouns and\n",
        "    #    adjectives only) that are connected if they occur in a window of\n",
        "    #    3 words.\n",
        "    extractor.candidate_weighting(window=3, pos=pos)\n",
        "    # 5. get the 5-highest scored candidates as keyphrases\n",
        "    keyphrases = extractor.get_n_best(n=5)\n",
        "    results = []\n",
        "    for keyword, score in keyphrases:\n",
        "        results.append(keyword)\n",
        "    return results \n",
        "\n",
        "position_rank_extractor(text)    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGDLjG-FN2s6",
        "outputId": "da53d263-3272-41d3-a616-d5526d8a4444"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dominant sequence transduction models',\n",
              " 'best models',\n",
              " 'models',\n",
              " 'convolutional neural networks',\n",
              " 'new simple network architecture']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from rake_nltk import Rake\n",
        "\n",
        "def rake_extractor(text):\n",
        "    \"\"\"\n",
        "    Uses Rake to extract the top 5 keywords from a text\n",
        "    Arguments: text (str)\n",
        "    Returns: list of keywords (list)\n",
        "    \"\"\"\n",
        "    r = Rake()\n",
        "    r.extract_keywords_from_text(text)\n",
        "    return r.get_ranked_phrases()[:5]\n",
        "\n",
        "rake_extractor(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpsPdkPYQBpR",
        "outputId": "cf262fe8-1891-4902-be8c-7b6c4d127bdd"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['best performing models also connect',\n",
              " 'two machine translation tasks show',\n",
              " 'requiring significantly less time',\n",
              " 'new simple network architecture',\n",
              " 'dominant sequence transduction models']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep-Learning-based methods\n",
        "\n",
        "The previous models typically work based on the statistical properties of a text and not so much on semantic similarity.\n",
        "\n",
        "The appearance of deep learning has enabled embedding-based methods. Researchers have developed several keyword extraction methods that use document embeddings and enable the model to be based on the semantic similarity."
      ],
      "metadata": {
        "id": "-J_0chjESfXH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keyword Extraction using BERT\n",
        "\n",
        "It is a minimal and easy-to-use keyword extraction technique that leverages BERT embeddings.\n",
        "\n",
        "\n",
        "1.   **Candidate Keywords/Keyphrases**:\n",
        "    * Creating a list of candidate keywords or keyphrases from a document.\n",
        "    * `CountVectorizer`. This allows us to specify the length of the keywords and make them into keyphrases. It also is a nice method for quickly removing stop words.\n",
        "\n",
        "2.   **Embedding**: \n",
        "    * We use BERT for this purpose as it has shown great results for both semantic similarity and paraphrase.\n",
        "    *  **Distilbert** as it has shown great performance in similarity tasks, which is what we are aiming for with keyword/keyphrase extraction!\n",
        "\n",
        "3.   **Similarity**: \n",
        "    * Find the candidates that are most similar to the document. \n",
        "    * We will be using the **cosine similarity** between vectors\n",
        "\n"
      ],
      "metadata": {
        "id": "6Yx7IaZMTzM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# 1. Extract candidate words/phrases\n",
        "n_gram_range = (2, 2)\n",
        "stop_words = \"english\"\n",
        "\n",
        "count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([text])\n",
        "candidates = count.get_feature_names_out()\n",
        "\n",
        "# 2. Embedding\n",
        "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
        "doc_embedding = model.encode([text])\n",
        "candidate_embeddings = model.encode(candidates)\n",
        "\n",
        "# 3. Similarity\n",
        "top_n = 5\n",
        "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
        "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
        "\n",
        "keywords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1bFI5XIRJPJ",
        "outputId": "b537f8a5-5e43-4037-a2d4-dd321f774b76"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['machine translation',\n",
              " 'neural networks',\n",
              " 'convolutional neural',\n",
              " 'models superior',\n",
              " 'decoder best']"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Refrences\n",
        "\n",
        "- [Keyword Extraction — A Benchmark of 7 Algorithms in Python](https://towardsdatascience.com/keyword-extraction-a-benchmark-of-7-algorithms-in-python-8a905326d93f)\n",
        "- [Keyword Extraction Methods — The Overview](https://towardsdatascience.com/keyword-extraction-methods-the-overview-35557350f8bb)\n",
        "- [Keyword Extraction process in Python with Natural Language Processing(NLP)](https://towardsdatascience.com/keyword-extraction-process-in-python-with-natural-language-processing-nlp-d769a9069d5c)\n",
        "- [Keyword Extraction with BERT](https://towardsdatascience.com/keyword-extraction-with-bert-724efca412ea)"
      ],
      "metadata": {
        "id": "PQa0gnO1bmt0"
      }
    }
  ]
}